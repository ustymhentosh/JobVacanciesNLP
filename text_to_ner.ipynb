{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e228cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gliner2 import GLiNER2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7b6718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/texts_ner'\n",
    "SCHEMA_PATH = 'data/annotated/schema.json'\n",
    "FOLDER_NAMES = ['Python', 'ML']\n",
    "SAVE_DIR = 'data/another_annotated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "832613f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans unwanted special characters from text, including '*', '#', and '‚óè',\n",
    "    normalizes spaces and newlines, and trims the text.\n",
    "    \"\"\"\n",
    "    # Remove bullet symbols anywhere in text\n",
    "    text = re.sub(r'[\\*\\#‚óè]', '', text)\n",
    "    # Normalize multiple spaces/tabs to single space\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Normalize multiple newlines to max two newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "texts = []\n",
    "for folder in FOLDER_NAMES:\n",
    "    folder_path = os.path.join(DATA_DIR, folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                raw_text = f.read()\n",
    "            cleaned = clean_text(raw_text)\n",
    "            texts.append({'filename': f\"{folder}/{file}\", 'text': cleaned})\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6afc2126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SKILL_HARD': \"Specific technical tools, programming languages, frameworks, or methodologies. Examples: 'Python', 'React.js', 'Docker', 'machine learning', 'REST API', 'CI/CD'\",\n",
       " 'SKILL_SOFT': \"Personal, communication, or team-related abilities that describe behavioral or interpersonal skills. Examples: 'problem-solving', 'team player', 'attention to detail', 'leadership'\",\n",
       " 'ENGLISH_LEVEL': \"Any explicit or implied mention of English proficiency or fluency level. Examples: 'Upper-Intermediate', 'fluent English', 'B2 level', 'advanced English communication'\",\n",
       " 'DEGREE': \"Formal education requirements or mentions of degree type or study field. Examples: 'Bachelor‚Äôs degree', 'Master‚Äôs in Computer Science', 'PhD in Engineering'\",\n",
       " 'EXPERIENCE_LEVEL': \"Seniority or professional rank associated with the role. Examples: 'Junior', 'Middle', 'Senior', 'Lead', 'Intern', 'Principal Engineer'\",\n",
       " 'EXPERIENCE_YEARS': \"Duration or number of years of experience required or mentioned. Examples: '3+ years', 'at least two years'\",\n",
       " 'BENEFIT': \"Offered perks, bonuses, or employment benefits and work conditions. Examples: 'medical insurance', 'flexible schedule', 'remote work', 'paid vacation', 'stock options'\",\n",
       " 'LOCATION': \"Mention of workplace location or work setting type. Examples: 'Kyiv', 'Lviv', 'remote', 'Poland'\",\n",
       " 'COMPANY': \"Employer, recruiter, or company name appearing in the text. Examples: 'EPAM Systems', 'SoftServe', 'Google'\",\n",
       " 'ROLE': \"Job title or position name describing the primary role. Examples: 'Frontend Developer', 'DevOps Engineer', 'Data Scientist'\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SCHEMA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c188efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3e57917e504ee68936ce2f9a2a5e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/208 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--fastino--gliner2-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595202ef32ee415bb3edb6882667baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e417659ad11466ebbd77f559bcbfa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db50e65b43d44deb84aff53d06ba2d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b273b0c1f9074e4fa104c31b49649963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e624e57d0d4cd3acd04d41fccf5ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d9135c91134c1098ccda1c3b043de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß†  Model Configuration\n",
      "============================================================\n",
      "Encoder model      : microsoft/deberta-v3-base\n",
      "Counting layer     : count_lstm_v2\n",
      "Token pooling      : first\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d698e3327a2b49689a3a6c5196ac61ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/834M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9883cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [16:03<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def assign_offsets(text, entities):\n",
    "    \"\"\"\n",
    "    Assign start/end offsets for each entity mention in text,\n",
    "    returning a list of (start, end, label) tuples sorted by start.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    seen_spans = set()\n",
    "    \n",
    "    for ent in entities:\n",
    "        ent_text = ent['text'].strip()\n",
    "        label = ent['label'].upper()\n",
    "        if not ent_text:\n",
    "            continue\n",
    "        \n",
    "        # Word boundary regex for exact match, case-insensitive\n",
    "        pattern = r'\\b' + re.escape(ent_text) + r'\\b'\n",
    "        \n",
    "        for match in re.finditer(pattern, text, flags=re.IGNORECASE):\n",
    "            span = (match.start(), match.end())\n",
    "            if span not in seen_spans:\n",
    "                seen_spans.add(span)\n",
    "                spans.append((match.start(), match.end(), label))\n",
    "    \n",
    "    # Sort spans by start position\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    return spans\n",
    "\n",
    "\n",
    "def dict_to_entity_list(entities_dict):\n",
    "    entities_list = []\n",
    "    for label, texts in entities_dict.items():\n",
    "        for text in texts:\n",
    "            entities_list.append({\"text\": text, \"label\": label})\n",
    "    return entities_list\n",
    "\n",
    "annotations = []\n",
    "\n",
    "for item in tqdm(texts, desc='Extracting entities'):\n",
    "    result = model.extract_entities(item[\"text\"], labels)\n",
    "    entities_dict = result.get('entities', result)\n",
    "    entity_list = dict_to_entity_list(entities_dict)\n",
    "    entities_with_offsets = assign_offsets(item[\"text\"], entity_list)\n",
    "    annotations.append({\n",
    "        \"filename\": item[\"filename\"],\n",
    "        \"text\": item[\"text\"],\n",
    "        \"entities\": entities_with_offsets\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a908f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filename', 'text', 'entities'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in FOLDER_NAMES:\n",
    "    output_folder = os.path.join(SAVE_DIR, folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for ann in annotations:\n",
    "    # Extract folder from filename, e.g. 'Python/file.txt' ‚Üí 'Python'\n",
    "    folder = ann['filename'].split('/')[0]\n",
    "    output_folder = os.path.join(SAVE_DIR, folder)\n",
    "    filename_json = os.path.basename(ann['filename']).replace('.txt', '.json')\n",
    "    output_path = os.path.join(output_folder, filename_json)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ann, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77ffdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in FOLDER_NAMES:\n",
    "    os.makedirs(os.path.join(SAVE_DIR, folder), exist_ok=True)\n",
    "\n",
    "for ann in annotations:\n",
    "    folder = ann['filename'].split('/')[0]\n",
    "    output_folder = os.path.join(SAVE_DIR, folder)\n",
    "    filename_json = os.path.basename(ann['filename']).replace('.txt', '.json')\n",
    "    output_path = os.path.join(output_folder, filename_json)\n",
    "\n",
    "    data_to_save = {\n",
    "        \"classes\": list(labels.keys()),\n",
    "        \"annotations\": [\n",
    "            [\n",
    "                ann[\"text\"],\n",
    "                {\"entities\": [[start, end, label] for start, end, label in ann[\"entities\"]]}\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_to_save, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
