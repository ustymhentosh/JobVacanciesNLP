{
  "filename": "Python/20.txt",
  "text": "Senior Backend Engineer ‚Äî Data Infrastructure & Web Scraping\nüåç Location: remote\n\nWe‚Äôre seeking a backend engineer with deep expertise in web scraping and data\ninfrastructure to build the foundational systems with one of our partners, an\nIsraeli startup product.\n\nYou‚Äôll be responsible for designing and implementing scalable pipelines that\ncollect, process, and serve data from complex platforms at enterprise scale.\nDay-to-Day Responsibilities\n\nDevelop large-scale scraping routines, tackling challenges of performance, accuracy, and source accessibility\nCreate and optimize data processing pipelines in Python, structuring high-performance databases and queries (SQL)\nWork in AWS (cloud-native), with flexibility to adapt solutions to other clouds\nParticipate in technical discussions with the founders, contributing to architectural and product decisions\nApply creativity and critical thinking to solve unprecedented problems without ready-made playbooks\nRequired Qualifications\n\nWeb Scraping Expertise:\n\nProven experience with large-scale web scraping on complex platforms like Reddit or Wikipedia\nSkilled at traversing multi-level link structures and handling edge cases\nAbility to bypass anti-scraping measures and partial-access restrictions (books, journals, paywalls)\nAdvanced scraping capabilities, including complex crawling and parsing of multiple formats\n\nBackend Development:\n\nStrong Python skills for backend processing, scripting, and automation\nExperience parsing, cleaning, and structuring data from APIs and scraped sources\nProven ability to prepare large datasets accurately and completely, including small or hard-to-reach sources\n\nData & Infrastructure:\n\nSQL proficiency for querying, transforming, and database modeling\nExperience handling large volumes of data with low-latency and high-precision requirements\nExperience designing scalable data pipelines for collection, storage, and retrieval\n\nAPI Development:\n\nExperience building robust B2B APIs that expose processed datasets with layered metadata\nUnderstanding of performance optimization for enterprise-scale API delivery\n\nCloud Infrastructure:\n\nExperience with AWS (adaptable to other cloud platforms)\nKnowledge of cloud-native architecture and security best practices\nNice to Have\n\nFamiliarity with LLMs and Prompt Engineering for validating sources or assessing scrappability\nExperience with RPA and data collection automation\nKnowledge of JavaScript or other languages for specific integrations\nUnderstanding of distributed architecture and resilient system design\nExperience with natural language processing for content analysis\nBackground in statistical modeling for trust scoring or similar applications\nTimeline & Impact\n\nTimeline: Minimum 6-month development cycle to deliver a production-ready\nMVP with enterprise customers including major search engines and AI platforms.\n\nImpact: The technical architecture you build must handle massive data\nprocessing, efficient cross-referencing, automated content verification, and\nenterprise-scale API delivery. Your work will directly enable AI systems\nworldwide to serve more reliable information.",
  "entities": [
    [
      0,
      6,
      "EXPERIENCE_LEVEL"
    ],
    [
      7,
      23,
      "ROLE"
    ],
    [
      73,
      79,
      "LOCATION"
    ],
    [
      97,
      113,
      "ROLE"
    ],
    [
      605,
      611,
      "SKILL_HARD"
    ],
    [
      665,
      668,
      "SKILL_HARD"
    ],
    [
      678,
      681,
      "SKILL_HARD"
    ],
    [
      879,
      896,
      "SKILL_SOFT"
    ],
    [
      1398,
      1404,
      "SKILL_HARD"
    ],
    [
      1677,
      1680,
      "SKILL_HARD"
    ],
    [
      2142,
      2145,
      "SKILL_HARD"
    ],
    [
      2281,
      2285,
      "SKILL_HARD"
    ],
    [
      2375,
      2378,
      "SKILL_HARD"
    ],
    [
      2423,
      2433,
      "SKILL_HARD"
    ],
    [
      2565,
      2592,
      "SKILL_HARD"
    ]
  ]
}