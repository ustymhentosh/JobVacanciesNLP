{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b92924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Span\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab69e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().resolve().parents[2]\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'cleaned_standard_annotated')\n",
    "SAVE_SPACY = os.path.join(BASE_DIR, 'data', 'spacy')\n",
    "CLEANTED_TEXTS_DIR = os.path.join(BASE_DIR, 'data', 'texts_ner_cleaned_standard')\n",
    "SCHEMA_PATH = os.path.join(BASE_DIR, 'data', 'annotated_manual', 'schema.json')\n",
    "FOLDER_NAMES = ['Python', 'ML', 'Android', 'DevOps', 'dotNET', 'FrontEnd', 'Golang', 'Java', 'macOS', 'Node', 'PHP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d986928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 702 annotated samples\n"
     ]
    }
   ],
   "source": [
    "# Loading annotated NER data\n",
    "def load_annotated_data(base_dir):\n",
    "    data = []\n",
    "    for folder in FOLDER_NAMES:\n",
    "        folder_path = os.path.join(DATA_DIR, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".json\"):\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        file = json.load(f)\n",
    "                        text = file['text']\n",
    "                        entities = [\n",
    "                            (start, end, label)\n",
    "                            for start, end, label in file.get(\"entities\", [])\n",
    "                        ]\n",
    "                        if text.strip():\n",
    "                            data.append((text, {\"entities\": entities}))\n",
    "\n",
    "    print(f\"Loaded {len(data)} annotated samples\")\n",
    "    return data\n",
    "\n",
    "data = load_annotated_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6dd5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [(0, 13, 'EXPERIENCE_LEVEL'), (0, 6, 'EXPERIENCE_LEVEL'), (7, 13, 'EXPERIENCE_LEVEL'), (14, 33, 'ROLE'), (39, 44, 'COMPANY_NAME'), (466, 479, 'EXPERIENCE_LEVEL'), (466, 472, 'EXPERIENCE_LEVEL'), (473, 479, 'EXPERIENCE_LEVEL'), (480, 499, 'ROLE'), (733, 737, 'EXPERIENCE_LEVEL'), (1165, 1175, 'EXPERIENCE_YEARS'), (1226, 1232, 'SKILL_HARD'), (1234, 1238, 'SKILL_HARD'), (1243, 1249, 'SKILL_HARD'), (1251, 1254, 'SKILL_HARD'), (1287, 1297, 'SKILL_HARD'), (1299, 1304, 'SKILL_HARD'), (1308, 1314, 'SKILL_HARD'), (1358, 1369, 'SKILL_HARD'), (1424, 1434, 'SKILL_HARD'), (1509, 1512, 'SKILL_HARD'), (1859, 1871, 'BENEFIT'), (1940, 1950, 'BENEFIT'), (1967, 1983, 'BENEFIT'), (2013, 2021, 'BENEFIT'), (2105, 2124, 'BENEFIT'), (2127, 2132, 'COMPANY_NAME'), (2160, 2164, 'EXPERIENCE_LEVEL'), (2244, 2252, 'BENEFIT'), (2309, 2317, 'BENEFIT'), (2608, 2614, 'LOCATION'), (2622, 2637, 'LOCATION')]}\n",
      "0 13 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "0 6 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "7 13 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "14 33 ROLE\n",
      "ROLE\n",
      "39 44 COMPANY_NAME\n",
      "COMPANY_NAME\n",
      "466 479 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "466 472 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "473 479 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "480 499 ROLE\n",
      "ROLE\n",
      "733 737 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "1165 1175 EXPERIENCE_YEARS\n",
      "EXPERIENCE_YEARS\n",
      "1226 1232 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1234 1238 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1243 1249 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1251 1254 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1287 1297 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1299 1304 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1308 1314 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1358 1369 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1424 1434 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1509 1512 SKILL_HARD\n",
      "SKILL_HARD\n",
      "1859 1871 BENEFIT\n",
      "BENEFIT\n",
      "1940 1950 BENEFIT\n",
      "BENEFIT\n",
      "1967 1983 BENEFIT\n",
      "BENEFIT\n",
      "2013 2021 BENEFIT\n",
      "BENEFIT\n",
      "2105 2124 BENEFIT\n",
      "BENEFIT\n",
      "2127 2132 COMPANY_NAME\n",
      "COMPANY_NAME\n",
      "2160 2164 EXPERIENCE_LEVEL\n",
      "EXPERIENCE_LEVEL\n",
      "2244 2252 BENEFIT\n",
      "BENEFIT\n",
      "2309 2317 BENEFIT\n",
      "BENEFIT\n",
      "2608 2614 LOCATION\n",
      "LOCATION\n",
      "2622 2637 LOCATION\n",
      "LOCATION\n"
     ]
    }
   ],
   "source": [
    "# Example of one sample\n",
    "for text, item in data[:1]:\n",
    "    print(item)\n",
    "    for start, end, label in item['entities']:\n",
    "        print(start, end, label)\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cf8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docbin_data(nlp, data):\n",
    "    '''\n",
    "    Converting text and entities into DocBin Spacy format\n",
    "    '''\n",
    "    db = DocBin()\n",
    "    for text, item in data:\n",
    "        doc = nlp(text)\n",
    "        spans = []\n",
    "        for start_char, end_char, label in item['entities']:\n",
    "            # Find the start token index\n",
    "            token_start = None\n",
    "            token_end = None\n",
    "            for token in doc:\n",
    "                if token.idx <= start_char < token.idx + len(token):\n",
    "                    token_start = token.i\n",
    "                if token.idx < end_char <= token.idx + len(token):\n",
    "                    token_end = token.i + 1\n",
    "            if token_start is not None and token_end is not None:\n",
    "                span = Span(doc, token_start, token_end, label=label)\n",
    "                spans.append(span)\n",
    "            else:\n",
    "                print(f\"Skipping invalid span: {text[start_char:end_char]} at ({start_char}, {end_char})\")\n",
    "        filtered_spans = spacy.util.filter_spans(spans)\n",
    "        doc.ents = filtered_spans\n",
    "        db.add(doc)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141fcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into test and validation for spacy\n",
    "# Test data will be runed on inference\n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "dev_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "nlp = spacy.blank('en')\n",
    "os.makedirs(SAVE_SPACY, exist_ok=True)\n",
    "\n",
    "# Creating specific DocBin format\n",
    "train_db = make_docbin_data(nlp, train_data)\n",
    "dev_db = make_docbin_data(nlp, dev_data)\n",
    "test_db = make_docbin_data(nlp, test_data)\n",
    "\n",
    "# Saving\n",
    "train_db.to_disk(os.path.join(SAVE_SPACY, \"train.spacy\"))\n",
    "dev_db.to_disk(os.path.join(SAVE_SPACY, \"dev.spacy\"))\n",
    "test_db.to_disk(os.path.join(SAVE_SPACY, \"test.spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56388bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Programing\\kurs 4\\sem 1\\nlp\\project\\JobVacanciesNLP\\data\\spacy\\test.spacy\n"
     ]
    }
   ],
   "source": [
    "print(os.path.join(SAVE_SPACY, \"test.spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4cf51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting: D:\\Programing\\kurs 4\\sem 1\\nlp\\project\\JobVacanciesNLP\\data\\spacy\\train.spacy\n",
      "Docs: 561\n",
      "Total entities: 18159\n",
      "Inspecting: D:\\Programing\\kurs 4\\sem 1\\nlp\\project\\JobVacanciesNLP\\data\\spacy\\dev.spacy\n",
      "Docs: 70\n",
      "Total entities: 2292\n"
     ]
    }
   ],
   "source": [
    "def inspect(path):\n",
    "    print(\"Inspecting:\", path)\n",
    "    db = DocBin().from_disk(path)\n",
    "    n = 0\n",
    "    ents = 0\n",
    "    for doc in db.get_docs(spacy.blank(\"en\").vocab):\n",
    "        n += 1\n",
    "        ents += len(doc.ents)\n",
    "        if len(doc.ents) == 0:\n",
    "            print(\"EMPTY DOC:\", doc.text[:100])\n",
    "    print(\"Docs:\", n)\n",
    "    print(\"Total entities:\", ents)\n",
    "\n",
    "inspect(os.path.join(SAVE_SPACY, \"train.spacy\"))\n",
    "inspect(os.path.join(SAVE_SPACY, \"dev.spacy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5415dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config_transformer.cfg --output ./output --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e57d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def filter_overlapping_spans_keep_longest(spans):\n",
    "#     # Sort spans by start index, then by length descending (longest first)\n",
    "#     sorted_spans = sorted(spans, key=lambda span: (span.start, -(span.end - span.start)))\n",
    "#     filtered = []\n",
    "#     token_owners = {}  # token index -> span that owns it\n",
    "\n",
    "#     for span in sorted_spans:\n",
    "#         overlap = False\n",
    "#         for token_idx in range(span.start, span.end):\n",
    "#             if token_idx in token_owners:\n",
    "#                 # Check if current span is longer than the one owning this token\n",
    "#                 owner = token_owners[token_idx]\n",
    "#                 owner_len = owner.end - owner.start\n",
    "#                 span_len = span.end - span.start\n",
    "#                 if span_len <= owner_len:\n",
    "#                     overlap = True\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     # Current span longer, remove owner span tokens from token_owners\n",
    "#                     for t in range(owner.start, owner.end):\n",
    "#                         if token_owners.get(t) == owner:\n",
    "#                             del token_owners[t]\n",
    "#                     # Also remove owner from filtered list\n",
    "#                     if owner in filtered:\n",
    "#                         filtered.remove(owner)\n",
    "#                     break\n",
    "#         if not overlap:\n",
    "#             for token_idx in range(span.start, span.end):\n",
    "#                 token_owners[token_idx] = span\n",
    "#             filtered.append(span)\n",
    "\n",
    "#     return filtered\n",
    "\n",
    "# nlp = spacy.blank('en')\n",
    "# db = DocBin()\n",
    "\n",
    "# for text, item in data:\n",
    "#     doc = nlp(text)\n",
    "#     spans = []\n",
    "#     for start_char, end_char, label in item['entities']:\n",
    "#         span = doc.char_span(start_char, end_char, label=label, alignment_mode='expand')\n",
    "#         if span is not None:\n",
    "#             spans.append(span)\n",
    "#         else:\n",
    "#             print(f\"Skipping invalid span: {text[start_char:end_char]} at ({start_char}, {end_char})\")\n",
    "#     spans = filter_overlapping_spans_keep_longest(spans)\n",
    "\n",
    "#     doc.ents = spans\n",
    "#     db.add(doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
